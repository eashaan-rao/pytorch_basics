{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note\n",
    "In the few upcoming notebooks we will understand the basic machine learning workflow which consists of:\n",
    "\n",
    "    1. Working with Data - Tensorsm, Datasets, DataLoaders and Transforms\n",
    "\n",
    "    2. Creating Models - Building the neural network, understanding the automatic differentiation using torch.autograd\n",
    "\n",
    "    3. Optimizing Model Parameters\n",
    "\n",
    "    4. Save and Load Model.\n",
    "\n",
    "All the scripts will be written in .ipynb files and using pytorch api.\n",
    "\n",
    "Dataset used: FashionMNIST\n",
    "Tutorial Link: https://pytorch.org/tutorials/beginner/basics/intro.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Working with data\n",
    "\n",
    "PyTorch has two methods to work with data: 'torch.utils.data.DataLoader' and 'torch.utils.data.Dataset'. Dataset stores the samples and labels while DataLoader is an iterable object which wraps about the Dataset object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every TorchVision \"Dataset\" includes two arguments - \"transform\" and \"target_transform\" - to modify the samples and labels respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download training data from open datasets\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"./data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")\n",
    "\n",
    "# Download test data from open datasets\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"./data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we pass \"Dataset\" as an argument to \"DataLoader\". This wraps an iterable over our FashionMNIST dataset and supports the following operation:\n",
    "\n",
    "    a) automatic batching \n",
    "\n",
    "    b) Sampling \n",
    "\n",
    "    c) shuffling \n",
    "\n",
    "    d) multiprocess data loading "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
      "Shape of y: torch.Size([64]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "# Create data loaders\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "for X, y in test_dataloader:\n",
    "    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
    "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Creating Models\n",
    "\n",
    "In Pytorch, we define a neural network with a help of a class. The class will inherits from nn.Module and consists of two methods __inti__() and forward(). __init__() define the layers of the network. foward() specify how data wil pass through the network. \n",
    "\n",
    "To accelerate the processing in neural nets, we can use CUDA, MPS, MTIA or XPU, these are called accelerator. If nothing is specified then it will choose CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using  cuda device\n"
     ]
    }
   ],
   "source": [
    "device = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else \"cpu\"\n",
    "\n",
    "print(f\"Using  {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "__init__(): is a special method called 'constructor', it is automatically create when NeuralNetwork object is created. Here \n",
    "            we defined the layers of neural network\n",
    "\n",
    "self - is a parameter and refers to the instance of the object (or current object)\n",
    "\n",
    "super().__init__() - calls the method of the parent class (nn.Module). It's required to properly initialize everything from\n",
    "                    nn.Module, like tracking layers and parameters\n",
    "\n",
    "self.flatten - it is layer called flatten. nn.Flatten() converts a multi-dimension input (e.g. 28 x 28 image) into a 1-d vector.\n",
    "               it is required because nn.Layer work with 1D inputs.\n",
    "\n",
    "self.linear_relu_stack = nn.Sequential() - defined the sequences of layers in the neural net. nn.Sequential allows to combine\n",
    "                                           multiple layers into a single block\n",
    "\n",
    "nn.Linear(28 * 28, 512) - it is a fully connected layer flattened to 784 inputs and ouputs 512 features.\n",
    "nn.ReLU() - is a activation function which introduces non-Linearity, it replaces negative values in output with zero.\n",
    "nn.Linear(512, 512) and nn.ReLU() - fully connected layer with 512 inputs and 512 outputs followed ReLU activation function\n",
    "nn.Linear(512, 10) -  final fully connected layer where 512 features are inputs and outputs 10 features. It is corresponds\n",
    "                      to 10 possible classes\n",
    "'''\n",
    "# Define model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28 * 28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10)\n",
    "        )\n",
    "    '''\n",
    "    def forward: defined forward pass of the network, i.e. how input data flows through networks to produce an output\n",
    "    x - represents input data\n",
    "    self.flatten - converts to 1D vector\n",
    "    self.linear_relu_stack = flattened input passed through network defined earler and final outputs stored in logits.\n",
    "    '''\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Optimizing the Model Parameters\n",
    "\n",
    "to train a model we need a loss function and an optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
