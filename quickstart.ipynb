{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note\n",
    "In the few upcoming notebooks we will understand the basic machine learning workflow which consists of:\n",
    "\n",
    "    1. Working with Data - Tensorsm, Datasets, DataLoaders and Transforms\n",
    "\n",
    "    2. Creating Models - Building the neural network, understanding the automatic differentiation using torch.autograd\n",
    "\n",
    "    3. Optimizing Model Parameters\n",
    "\n",
    "    4. Save and Load Model.\n",
    "\n",
    "All the scripts will be written in .ipynb files and using pytorch api.\n",
    "\n",
    "Dataset used: FashionMNIST\n",
    "Tutorial Link: https://pytorch.org/tutorials/beginner/basics/intro.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Working with data\n",
    "\n",
    "PyTorch has two methods to work with data: 'torch.utils.data.DataLoader' and 'torch.utils.data.Dataset'. Dataset stores the samples and labels while DataLoader is an iterable object which wraps about the Dataset object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every TorchVision \"Dataset\" includes two arguments - \"transform\" and \"target_transform\" - to modify the samples and labels respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download training data from open datasets\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"./data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")\n",
    "\n",
    "# Download test data from open datasets\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"./data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we pass \"Dataset\" as an argument to \"DataLoader\". This wraps an iterable over our FashionMNIST dataset and supports the following operation:\n",
    "\n",
    "    a) automatic batching \n",
    "\n",
    "    b) Sampling \n",
    "\n",
    "    c) shuffling \n",
    "\n",
    "    d) multiprocess data loading "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
      "Shape of y: torch.Size([64]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "# Create data loaders\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "for X, y in test_dataloader:\n",
    "    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
    "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Creating Models\n",
    "\n",
    "In Pytorch, we define a neural network with a help of a class. The class will inherits from nn.Module and consists of two methods __inti__() and forward(). __init__() define the layers of the network. foward() specify how data wil pass through the network. \n",
    "\n",
    "To accelerate the processing in neural nets, we can use CUDA, MPS, MTIA or XPU, these are called accelerator. If nothing is specified then it will choose CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using  cuda device\n"
     ]
    }
   ],
   "source": [
    "device = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else \"cpu\"\n",
    "\n",
    "print(f\"Using  {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "__init__(): is a special method called 'constructor', it is automatically create when NeuralNetwork object is created. Here \n",
    "            we defined the layers of neural network\n",
    "\n",
    "self - is a parameter and refers to the instance of the object (or current object)\n",
    "\n",
    "super().__init__() - calls the method of the parent class (nn.Module). It's required to properly initialize everything from\n",
    "                    nn.Module, like tracking layers and parameters\n",
    "\n",
    "self.flatten - it is layer called flatten. nn.Flatten() converts a multi-dimension input (e.g. 28 x 28 image) into a 1-d vector.\n",
    "               it is required because nn.Layer work with 1D inputs.\n",
    "\n",
    "self.linear_relu_stack = nn.Sequential() - defined the sequences of layers in the neural net. nn.Sequential allows to combine\n",
    "                                           multiple layers into a single block\n",
    "\n",
    "nn.Linear(28 * 28, 512) - it is a fully connected layer flattened to 784 inputs and ouputs 512 features.\n",
    "nn.ReLU() - is a activation function which introduces non-Linearity, it replaces negative values in output with zero.\n",
    "nn.Linear(512, 512) and nn.ReLU() - fully connected layer with 512 inputs and 512 outputs followed ReLU activation function\n",
    "nn.Linear(512, 10) -  final fully connected layer where 512 features are inputs and outputs 10 features. It is corresponds\n",
    "                      to 10 possible classes\n",
    "'''\n",
    "# Define model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28 * 28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10)\n",
    "        )\n",
    "    '''\n",
    "    def forward: defined forward pass of the network, i.e. how input data flows through networks to produce an output\n",
    "    x - represents input data\n",
    "    self.flatten - converts to 1D vector\n",
    "    self.linear_relu_stack = flattened input passed through network defined earler and final outputs stored in logits.\n",
    "    '''\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Optimizing the Model Parameters\n",
    "\n",
    "to train a model we need a loss function and an optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a single training loop, the model makes predictions on the training dataset (fed to it in batches), and backpropagates the prediction error to adjust the model's parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f} [{current:>5d}|{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method for testing the trained model\n",
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>.8f} \\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a typical process, each batches of data goes through several iterations (epochs) in the neural network. The purpose here to make model learns by adjusting the parameters to make better predictions. To know whether model is making correct prediction is based on loss function values, the model aims is to reduce the loss value. We can observe the model accuracy and loss at each epoch and we would like to see the accuracy increase and the loss decrease with every epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      " -------------------------------------------------\n",
      "loss: 2.303051 [   64|60000]\n",
      "loss: 2.295276 [ 6464|60000]\n",
      "loss: 2.277853 [12864|60000]\n",
      "loss: 2.276852 [19264|60000]\n",
      "loss: 2.246889 [25664|60000]\n",
      "loss: 2.219332 [32064|60000]\n",
      "loss: 2.236468 [38464|60000]\n",
      "loss: 2.195301 [44864|60000]\n",
      "loss: 2.206052 [51264|60000]\n",
      "loss: 2.164716 [57664|60000]\n",
      "Test Error: \n",
      " Accuracy: 36.0%, Avg loss: 2.16628930 \n",
      "\n",
      "Epoch 2\n",
      " -------------------------------------------------\n",
      "loss: 2.176670 [   64|60000]\n",
      "loss: 2.173671 [ 6464|60000]\n",
      "loss: 2.122136 [12864|60000]\n",
      "loss: 2.136147 [19264|60000]\n",
      "loss: 2.082084 [25664|60000]\n",
      "loss: 2.025294 [32064|60000]\n",
      "loss: 2.056125 [38464|60000]\n",
      "loss: 1.978423 [44864|60000]\n",
      "loss: 1.995018 [51264|60000]\n",
      "loss: 1.908728 [57664|60000]\n",
      "Test Error: \n",
      " Accuracy: 52.3%, Avg loss: 1.91812415 \n",
      "\n",
      "Epoch 3\n",
      " -------------------------------------------------\n",
      "loss: 1.955384 [   64|60000]\n",
      "loss: 1.931765 [ 6464|60000]\n",
      "loss: 1.823907 [12864|60000]\n",
      "loss: 1.847648 [19264|60000]\n",
      "loss: 1.745743 [25664|60000]\n",
      "loss: 1.694981 [32064|60000]\n",
      "loss: 1.707978 [38464|60000]\n",
      "loss: 1.611273 [44864|60000]\n",
      "loss: 1.639238 [51264|60000]\n",
      "loss: 1.515792 [57664|60000]\n",
      "Test Error: \n",
      " Accuracy: 59.4%, Avg loss: 1.54619356 \n",
      "\n",
      "Epoch 4\n",
      " -------------------------------------------------\n",
      "loss: 1.617806 [   64|60000]\n",
      "loss: 1.584425 [ 6464|60000]\n",
      "loss: 1.437864 [12864|60000]\n",
      "loss: 1.492393 [19264|60000]\n",
      "loss: 1.381061 [25664|60000]\n",
      "loss: 1.371631 [32064|60000]\n",
      "loss: 1.377521 [38464|60000]\n",
      "loss: 1.304199 [44864|60000]\n",
      "loss: 1.339054 [51264|60000]\n",
      "loss: 1.227687 [57664|60000]\n",
      "Test Error: \n",
      " Accuracy: 63.4%, Avg loss: 1.26408959 \n",
      "\n",
      "Epoch 5\n",
      " -------------------------------------------------\n",
      "loss: 1.344311 [   64|60000]\n",
      "loss: 1.326736 [ 6464|60000]\n",
      "loss: 1.164055 [12864|60000]\n",
      "loss: 1.258207 [19264|60000]\n",
      "loss: 1.137810 [25664|60000]\n",
      "loss: 1.160979 [32064|60000]\n",
      "loss: 1.175103 [38464|60000]\n",
      "loss: 1.117042 [44864|60000]\n",
      "loss: 1.153015 [51264|60000]\n",
      "loss: 1.062704 [57664|60000]\n",
      "Test Error: \n",
      " Accuracy: 65.0%, Avg loss: 1.09157750 \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n -------------------------------------------------\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    test(test_dataloader, model, loss_fn)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Saving Models\n",
    "\n",
    "A common way to save a model is to serialize internal state dictionary containing the model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Pytorch model state to model.pth\n"
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict(), \"model.pth\")\n",
    "print(\"Saved Pytorch model state to model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Loading Models\n",
    "The process for loading a model includes re-creating the model structure and loading the state dictionary into it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = NeuralNetwork().to(device)\n",
    "model.load_state_dict(torch.load(\"model.pth\", weights_only=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: \"Ankle boot\", Actual: \"Ankle boot\"\n"
     ]
    }
   ],
   "source": [
    "# Prediction code\n",
    "classes = [\n",
    "    \"T-shirt/top\",\n",
    "    \"Trouser\",\n",
    "    \"Pullover\",\n",
    "    \"Dress\",\n",
    "    \"Coat\",\n",
    "    \"Sandal\",\n",
    "    \"Shirt\",\n",
    "    \"Sneaker\",\n",
    "    \"Bag\",\n",
    "    \"Ankle boot\",\n",
    "]\n",
    "model.eval()\n",
    "x, y = test_data[0][0], test_data[0][1]\n",
    "with torch.no_grad():\n",
    "    x = x.to(device)\n",
    "    pred = model(x)\n",
    "    predicted, actual = classes[pred[0].argmax(0)], classes[y]\n",
    "    print(f'Predicted: \"{predicted}\", Actual: \"{actual}\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
